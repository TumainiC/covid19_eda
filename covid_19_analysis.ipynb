{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283fdfd6",
   "metadata": {},
   "source": [
    "# COVID-19 Research Data Analysis Project\n",
    "\n",
    "This notebook analyzes COVID-19 research papers from the CORD-19 dataset. We'll perform data loading, exploration, cleaning, analysis, and build a Streamlit application.\n",
    "\n",
    "## Project Overview\n",
    "- **Part 1**: Data Loading and Basic Exploration\n",
    "- **Part 2**: Data Cleaning and Preparation\n",
    "- **Part 3**: Data Analysis and Visualization\n",
    "- **Part 4**: Streamlit Application\n",
    "- **Part 5**: Documentation and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64520d",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "We'll start by importing all the necessary libraries for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3aefc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries for data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23647a66",
   "metadata": {},
   "source": [
    "## Section 2: Download and Load the Dataset\n",
    "\n",
    "We'll load the CORD-19 metadata.csv file into a pandas DataFrame. This dataset contains metadata about COVID-19 research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa93db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CORD-19 metadata dataset\n",
    "try:\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv('metadata.csv', low_memory=False)\n",
    "    print(f\" Dataset loaded successfully!\")\n",
    "    print(f\" Dataset shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    print(f\" Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\" metadata.csv not found. Please ensure the file is in the current directory.\")\n",
    "except Exception as e:\n",
    "    print(f\" Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bf462",
   "metadata": {},
   "source": [
    "## Section 3: Examine Data Structure and Basic Information\n",
    "\n",
    "Let's explore the structure of our dataset by examining the first few rows, column names, and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n Column names:\")\n",
    "\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\n Dataset dimensions: {df.shape[0]} rows Ã— {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"  Data types of each column:\")\n",
    "\n",
    "data_types = df.dtypes\n",
    "for col, dtype in data_types.items():\n",
    "    print(f\"{col:<25} {dtype}\")\n",
    "\n",
    "print(\"\\n General information:\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18257ed",
   "metadata": {},
   "source": [
    "## Section 4: Data Quality Assessment\n",
    "\n",
    "Now let's assess the quality of our data by checking for missing values and generating basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\" Missing values analysis:\")\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "\n",
    "# Create a summary dataframe\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing_Count': missing_data.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "# Sort by missing percentage\n",
    "missing_summary = missing_summary.sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "# Display columns with missing values\n",
    "missing_cols = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "print(f\" Columns with missing values: {len(missing_cols)} out of {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate basic statistics for key columns\n",
    "\n",
    "# Focus on key columns that are likely to be important\n",
    "key_columns = ['title', 'abstract', 'publish_time', 'journal', 'authors']\n",
    "\n",
    "for col in key_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(f\"  Non-null values: {df[col].notna().sum():,}\")\n",
    "        print(f\"  Unique values: {df[col].nunique():,}\")\n",
    "        if col == 'publish_time':\n",
    "            # Try to get date range\n",
    "            try:\n",
    "                dates = pd.to_datetime(df[col], errors='coerce')\n",
    "                print(f\"  Date range: {dates.min()} to {dates.max()}\")\n",
    "            except:\n",
    "                print(\"  Date conversion failed\")\n",
    "    else:\n",
    "        print(f\"\\n{col.upper()}: Column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94afbcd9",
   "metadata": {},
   "source": [
    "## Section 5: Handle Missing Data\n",
    "\n",
    "Based on our analysis, we'll create a cleaned version of the dataset by handling missing values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleaned version of the dataset\n",
    "\n",
    "# Start with a copy of the original data\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(f\"Original dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# For our analysis, we need papers with titles and some publication info\n",
    "# Remove rows where title is missing (these are not useful for our analysis)\n",
    "if 'title' in df_clean.columns:\n",
    "    df_clean = df_clean.dropna(subset=['title'])\n",
    "    print(f\"After removing rows without titles: {df_clean.shape[0]} rows\")\n",
    "\n",
    "# For publish_time, we'll keep all rows but handle missing dates later\n",
    "# For other columns, we'll fill missing values with appropriate defaults\n",
    "\n",
    "# Fill missing journal with 'Unknown'\n",
    "if 'journal' in df_clean.columns:\n",
    "    df_clean['journal'] = df_clean['journal'].fillna('Unknown')\n",
    "\n",
    "# Fill missing abstract with empty string\n",
    "if 'abstract' in df_clean.columns:\n",
    "    df_clean['abstract'] = df_clean['abstract'].fillna('')\n",
    "\n",
    "print(f\"Final cleaned dataset: {df_clean.shape[0]} rows, {df_clean.shape[1]} columns\")\n",
    "print(f\"Rows removed: {df.shape[0] - df_clean.shape[0]} ({((df.shape[0] - df_clean.shape[0]) / df.shape[0] * 100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6dfa2",
   "metadata": {},
   "source": [
    "## Section 6: Data Type Conversions and Feature Engineering\n",
    "\n",
    "Let's convert date columns and create new features for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b9b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert publish_time to datetime and extract year\n",
    "\n",
    "# Convert publish_time to datetime\n",
    "if 'publish_time' in df_clean.columns:\n",
    "    df_clean['publish_date'] = pd.to_datetime(df_clean['publish_time'], errors='coerce')\n",
    "    \n",
    "    # Extract year from publication date\n",
    "    df_clean['publish_year'] = df_clean['publish_date'].dt.year\n",
    "    \n",
    "    # For missing dates, we can try to extract year from the string if possible\n",
    "    missing_dates = df_clean['publish_date'].isna()\n",
    "    print(f\"Rows with missing dates: {missing_dates.sum()}\")\n",
    "    \n",
    "    # Check the date range\n",
    "    valid_dates = df_clean['publish_date'].dropna()\n",
    "    if len(valid_dates) > 0:\n",
    "        print(f\"Date range: {valid_dates.min()} to {valid_dates.max()}\")\n",
    "        print(f\"Year range: {df_clean['publish_year'].min()} to {df_clean['publish_year'].max()}\")\n",
    "\n",
    "# Create new features\n",
    "# Title length\n",
    "if 'title' in df_clean.columns:\n",
    "    df_clean['title_length'] = df_clean['title'].str.len()\n",
    "    print(f\"Title length - Mean: {df_clean['title_length'].mean():.1f}, Max: {df_clean['title_length'].max()}\")\n",
    "\n",
    "# Abstract word count\n",
    "if 'abstract' in df_clean.columns:\n",
    "    df_clean['abstract_word_count'] = df_clean['abstract'].str.split().str.len()\n",
    "    # Handle empty abstracts\n",
    "    df_clean['abstract_word_count'] = df_clean['abstract_word_count'].fillna(0)\n",
    "    print(f\"Abstract word count - Mean: {df_clean['abstract_word_count'].mean():.1f}, Max: {df_clean['abstract_word_count'].max()}\")\n",
    "\n",
    "print(f\"\\n Feature engineering complete! Dataset now has {df_clean.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290eccf9",
   "metadata": {},
   "source": [
    "## Section 7: Publication Trends Analysis\n",
    "\n",
    "Let's analyze publication trends over time to understand how COVID-19 research evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7680f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count papers by publication year\n",
    "\n",
    "if 'publish_year' in df_clean.columns:\n",
    "    # Count papers by year\n",
    "    yearly_counts = df_clean['publish_year'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"Papers published by year:\")\n",
    "    for year, count in yearly_counts.items():\n",
    "        if pd.notna(year):\n",
    "            print(f\"{year}: {count:,} papers\")\n",
    "    \n",
    "    # Focus on recent years (COVID-19 era)\n",
    "    covid_years = yearly_counts[yearly_counts.index >= 2019]\n",
    "    print(f\"\\nCOVID-19 era (2019+): {covid_years.sum():,} papers\")\n",
    "    \n",
    "    # Store for visualization\n",
    "    yearly_data = yearly_counts.reset_index()\n",
    "    yearly_data.columns = ['Year', 'Paper_Count']\n",
    "else:\n",
    "    print(\"No publication year data available\")\n",
    "\n",
    "# Analyze monthly trends for recent years\n",
    "if 'publish_date' in df_clean.columns:\n",
    "    print(\"\\n Monthly publication trends (2020-2022):\")\n",
    "    recent_papers = df_clean[df_clean['publish_year'].between(2020, 2022)]\n",
    "    if len(recent_papers) > 0:\n",
    "        monthly_counts = recent_papers.groupby([\n",
    "            recent_papers['publish_date'].dt.year,\n",
    "            recent_papers['publish_date'].dt.month\n",
    "        ]).size().reset_index(name='count')\n",
    "        \n",
    "        print(f\"Peak months analysis for recent COVID-19 research:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e6a92",
   "metadata": {},
   "source": [
    "## Section 8: Journal and Source Analysis\n",
    "\n",
    "Let's identify the top journals and sources publishing COVID-19 research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d234ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top journals publishing COVID-19 research\n",
    "\n",
    "\n",
    "if 'journal' in df_clean.columns:\n",
    "    # Count papers by journal\n",
    "    journal_counts = df_clean['journal'].value_counts()\n",
    "    \n",
    "    print(\"Top 15 journals by paper count:\")\n",
    "    for i, (journal, count) in enumerate(journal_counts.head(15).items(), 1):\n",
    "        print(f\"{i:2d}. {journal:<50} {count:>6,} papers\")\n",
    "    \n",
    "    # Store top journals for visualization\n",
    "    top_journals = journal_counts.head(10).reset_index()\n",
    "    top_journals.columns = ['Journal', 'Paper_Count']\n",
    "    \n",
    "    print(f\"\\nTotal unique journals: {len(journal_counts)}\")\n",
    "    print(f\"Papers in top 10 journals: {top_journals['Paper_Count'].sum():,} ({top_journals['Paper_Count'].sum()/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# Check if there are other source columns\n",
    "potential_source_cols = [col for col in df_clean.columns if 'source' in col.lower()]\n",
    "if potential_source_cols:\n",
    "    print(f\"\\n Other source columns found: {potential_source_cols}\")\n",
    "    \n",
    "    for col in potential_source_cols[:2]:  # Analyze first 2 source columns\n",
    "        print(f\"\\nTop sources in '{col}':\")\n",
    "        source_counts = df_clean[col].value_counts()\n",
    "        for i, (source, count) in enumerate(source_counts.head(10).items(), 1):\n",
    "            print(f\"{i:2d}. {source:<40} {count:>6,}\")\n",
    "else:\n",
    "    print(\"No additional source columns found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f6395",
   "metadata": {},
   "source": [
    "## Section 9: Text Analysis of Paper Titles\n",
    "\n",
    "Let's analyze the most common words and themes in COVID-19 research paper titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ef4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze word frequency in paper titles\n",
    "\n",
    "if 'title' in df_clean.columns:\n",
    "    # Get all titles and convert to lowercase\n",
    "    all_titles = df_clean['title'].dropna().str.lower()\n",
    "    \n",
    "    # Split titles into words and count frequency\n",
    "    all_words = []\n",
    "    for title in all_titles:\n",
    "        # Simple word extraction (remove punctuation and split)\n",
    "        words = title.replace(',', ' ').replace('.', ' ').replace(':', ' ').replace(';', ' ').replace('(', ' ').replace(')', ' ').split()\n",
    "        # Filter out very short words and common stop words\n",
    "        stop_words = {'the', 'and', 'of', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'with', 'by', 'from', 'as', 'is', 'are', 'was', 'were', 'be', 'been', 'being'}\n",
    "        words = [word.strip('.,!?:;()[]{}') for word in words if len(word) > 2 and word not in stop_words]\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(all_words)\n",
    "    \n",
    "    print(\"Top 20 most frequent words in titles:\")\n",
    "    for i, (word, count) in enumerate(word_counts.most_common(20), 1):\n",
    "        print(f\"{i:2d}. {word:<15} {count:>6,} times\")\n",
    "    \n",
    "    # Store for word cloud\n",
    "    top_words = dict(word_counts.most_common(50))\n",
    "    \n",
    "    # Analyze COVID-related terms\n",
    "    covid_terms = ['covid', 'coronavirus', 'sars', 'pandemic', 'vaccine', 'vaccination']\n",
    "    print(f\"\\nCOVID-19 related terms in titles:\")\n",
    "    for term in covid_terms:\n",
    "        count = word_counts.get(term, 0)\n",
    "        if count > 0:\n",
    "            print(f\"  {term}: {count:,} times\")\n",
    "    \n",
    "    print(f\"\\nTotal unique words: {len(word_counts)}\")\n",
    "    print(f\"Total words analyzed: {len(all_words):,}\")\n",
    "else:\n",
    "    print(\"No title data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6213cbe",
   "metadata": {},
   "source": [
    "## Section 10: Create Data Visualizations\n",
    "\n",
    "Now let's create visualizations to better understand our data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication trends visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 1. Publications over time\n",
    "plt.subplot(2, 2, 1)\n",
    "if 'yearly_data' in locals():\n",
    "    # Filter for reasonable years\n",
    "    yearly_filtered = yearly_data[yearly_data['Year'].between(2000, 2024)]\n",
    "    plt.plot(yearly_filtered['Year'], yearly_filtered['Paper_Count'], marker='o', linewidth=2, markersize=6)\n",
    "    plt.title('COVID-19 Research Publications by Year', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# 2. Top journals bar chart\n",
    "plt.subplot(2, 2, 2)\n",
    "if 'top_journals' in locals():\n",
    "    # Get top 8 journals for better readability\n",
    "    top_8_journals = top_journals.head(8)\n",
    "    bars = plt.bar(range(len(top_8_journals)), top_8_journals['Paper_Count'], color='skyblue', edgecolor='navy')\n",
    "    plt.title('Top Journals Publishing COVID-19 Research', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Journals')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.xticks(range(len(top_8_journals)), [j[:20] + '...' if len(j) > 20 else j for j in top_8_journals['Journal']], \n",
    "               rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{int(height):,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 3. Title length distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "if 'title_length' in df_clean.columns:\n",
    "    title_lengths = df_clean['title_length'].dropna()\n",
    "    plt.hist(title_lengths, bins=50, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "    plt.title('Distribution of Paper Title Lengths', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Title Length (characters)')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.axvline(title_lengths.mean(), color='red', linestyle='--', label=f'Mean: {title_lengths.mean():.1f}')\n",
    "    plt.legend()\n",
    "\n",
    "# 4. Abstract word count distribution\n",
    "plt.subplot(2, 2, 4)\n",
    "if 'abstract_word_count' in df_clean.columns:\n",
    "    abstract_counts = df_clean['abstract_word_count'].dropna()\n",
    "    # Filter out extreme outliers for better visualization\n",
    "    filtered_counts = abstract_counts[abstract_counts <= abstract_counts.quantile(0.95)]\n",
    "    plt.hist(filtered_counts, bins=50, color='orange', edgecolor='darkorange', alpha=0.7)\n",
    "    plt.title('Distribution of Abstract Word Counts', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Abstract Word Count')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.axvline(filtered_counts.mean(), color='red', linestyle='--', label=f'Mean: {filtered_counts.mean():.1f}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a33f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple word cloud visualization using matplotlib\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Word cloud alternative using bar chart\n",
    "plt.subplot(1, 2, 1)\n",
    "if 'top_words' in locals():\n",
    "    # Get top 15 words for better readability\n",
    "    top_15_words = dict(list(top_words.items())[:15])\n",
    "    words = list(top_15_words.keys())\n",
    "    counts = list(top_15_words.values())\n",
    "    \n",
    "    bars = plt.barh(range(len(words)), counts, color='lightcoral', edgecolor='darkred')\n",
    "    plt.title('Most Frequent Words in Paper Titles', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.gca().invert_yaxis()  # Most frequent at top\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + width*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{int(width):,}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "# COVID-19 related terms visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "if 'word_counts' in locals():\n",
    "    covid_related = ['covid', 'coronavirus', 'sars', 'pandemic', 'vaccine', 'vaccination', 'treatment', 'patient', 'clinical']\n",
    "    covid_data = [(word, word_counts.get(word, 0)) for word in covid_related if word_counts.get(word, 0) > 0]\n",
    "    \n",
    "    if covid_data:\n",
    "        covid_words, covid_counts = zip(*covid_data)\n",
    "        bars = plt.bar(range(len(covid_words)), covid_counts, color='steelblue', edgecolor='navy')\n",
    "        plt.title('COVID-19 Related Terms in Titles', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Terms')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(range(len(covid_words)), covid_words, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{int(height):,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "\n",
    "# Saving only a sample to avaoid large file sizes\n",
    "df_clean.sample(frac=0.1).to_csv('covid19_cleaned_sample.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
