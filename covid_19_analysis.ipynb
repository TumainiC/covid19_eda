{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283fdfd6",
   "metadata": {},
   "source": [
    "# COVID-19 Research Data Analysis Project\n",
    "\n",
    "This notebook analyzes COVID-19 research papers from the CORD-19 dataset. We'll perform data loading, exploration, cleaning, analysis, and build a Streamlit application.\n",
    "\n",
    "## Project Overview\n",
    "- **Part 1**: Data Loading and Basic Exploration\n",
    "- **Part 2**: Data Cleaning and Preparation\n",
    "- **Part 3**: Data Analysis and Visualization\n",
    "- **Part 4**: Streamlit Application\n",
    "- **Part 5**: Documentation and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64520d",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "We'll start by importing all the necessary libraries for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3aefc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries for data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23647a66",
   "metadata": {},
   "source": [
    "## Section 2: Download and Load the Dataset\n",
    "\n",
    "We'll load the CORD-19 metadata.csv file into a pandas DataFrame. This dataset contains metadata about COVID-19 research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa93db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset loaded successfully!\n",
      " Dataset shape: 1056660 rows, 19 columns\n",
      " Memory usage: 2688.97 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the CORD-19 metadata dataset\n",
    "try:\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv('metadata.csv', low_memory=False)\n",
    "    print(f\" Dataset loaded successfully!\")\n",
    "    print(f\" Dataset shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    print(f\" Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\" metadata.csv not found. Please ensure the file is in the current directory.\")\n",
    "except Exception as e:\n",
    "    print(f\" Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bf462",
   "metadata": {},
   "source": [
    "## Section 3: Examine Data Structure and Basic Information\n",
    "\n",
    "Let's explore the structure of our dataset by examining the first few rows, column names, and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e648021c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>mag_id</th>\n",
       "      <th>who_covidence_id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>pdf_json_files</th>\n",
       "      <th>pmc_json_files</th>\n",
       "      <th>url</th>\n",
       "      <th>s2_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ug7v899j</td>\n",
       "      <td>d1aafb70c066a2068b02786f8929fd9c900897fb</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
       "      <td>10.1186/1471-2334-1-6</td>\n",
       "      <td>PMC35282</td>\n",
       "      <td>11472636</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>OBJECTIVE: This retrospective chart review des...</td>\n",
       "      <td>2001-07-04</td>\n",
       "      <td>Madani, Tariq A; Al-Ghamdi, Aisha A</td>\n",
       "      <td>BMC Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/d1aafb70c066a2068b027...</td>\n",
       "      <td>document_parses/pmc_json/PMC35282.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02tnwd4m</td>\n",
       "      <td>6b0567729c2143a66d737eb0a2f63f2dce2e5a7d</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
       "      <td>10.1186/rr14</td>\n",
       "      <td>PMC59543</td>\n",
       "      <td>11667967</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Inflammatory diseases of the respiratory tract...</td>\n",
       "      <td>2000-08-15</td>\n",
       "      <td>Vliet, Albert van der; Eiserich, Jason P; Cros...</td>\n",
       "      <td>Respir Res</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/6b0567729c2143a66d737...</td>\n",
       "      <td>document_parses/pmc_json/PMC59543.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ejv2xln0</td>\n",
       "      <td>06ced00a5fc04215949aa72528f2eeaae1d58927</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Surfactant protein-D and pulmonary host defense</td>\n",
       "      <td>10.1186/rr19</td>\n",
       "      <td>PMC59549</td>\n",
       "      <td>11667972</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Surfactant protein-D (SP-D) participates in th...</td>\n",
       "      <td>2000-08-25</td>\n",
       "      <td>Crouch, Erika C</td>\n",
       "      <td>Respir Res</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/06ced00a5fc04215949aa...</td>\n",
       "      <td>document_parses/pmc_json/PMC59549.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2b73a28n</td>\n",
       "      <td>348055649b6b8cf2b9a376498df9bf41f7123605</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Role of endothelin-1 in lung disease</td>\n",
       "      <td>10.1186/rr44</td>\n",
       "      <td>PMC59574</td>\n",
       "      <td>11686871</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n",
       "      <td>2001-02-22</td>\n",
       "      <td>Fagan, Karen A; McMurtry, Ivan F; Rodman, David M</td>\n",
       "      <td>Respir Res</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/348055649b6b8cf2b9a37...</td>\n",
       "      <td>document_parses/pmc_json/PMC59574.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9785vg6d</td>\n",
       "      <td>5f48792a5fa08bed9f56016f4981ae2ca6031b32</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Gene expression in epithelial cells in respons...</td>\n",
       "      <td>10.1186/rr61</td>\n",
       "      <td>PMC59580</td>\n",
       "      <td>11686888</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Respiratory syncytial virus (RSV) and pneumoni...</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>Domachowske, Joseph B; Bonville, Cynthia A; Ro...</td>\n",
       "      <td>Respir Res</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/5f48792a5fa08bed9f560...</td>\n",
       "      <td>document_parses/pmc_json/PMC59580.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb      PMC   \n",
       "1  02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d      PMC   \n",
       "2  ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927      PMC   \n",
       "3  2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605      PMC   \n",
       "4  9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32      PMC   \n",
       "\n",
       "                                               title                    doi  \\\n",
       "0  Clinical features of culture-proven Mycoplasma...  10.1186/1471-2334-1-6   \n",
       "1  Nitric oxide: a pro-inflammatory mediator in l...           10.1186/rr14   \n",
       "2    Surfactant protein-D and pulmonary host defense           10.1186/rr19   \n",
       "3               Role of endothelin-1 in lung disease           10.1186/rr44   \n",
       "4  Gene expression in epithelial cells in respons...           10.1186/rr61   \n",
       "\n",
       "      pmcid pubmed_id license  \\\n",
       "0  PMC35282  11472636   no-cc   \n",
       "1  PMC59543  11667967   no-cc   \n",
       "2  PMC59549  11667972   no-cc   \n",
       "3  PMC59574  11686871   no-cc   \n",
       "4  PMC59580  11686888   no-cc   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
       "1  Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
       "2  Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
       "3  Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
       "4  Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
       "\n",
       "                                             authors         journal  mag_id  \\\n",
       "0                Madani, Tariq A; Al-Ghamdi, Aisha A  BMC Infect Dis     NaN   \n",
       "1  Vliet, Albert van der; Eiserich, Jason P; Cros...      Respir Res     NaN   \n",
       "2                                    Crouch, Erika C      Respir Res     NaN   \n",
       "3  Fagan, Karen A; McMurtry, Ivan F; Rodman, David M      Respir Res     NaN   \n",
       "4  Domachowske, Joseph B; Bonville, Cynthia A; Ro...      Respir Res     NaN   \n",
       "\n",
       "  who_covidence_id arxiv_id  \\\n",
       "0              NaN      NaN   \n",
       "1              NaN      NaN   \n",
       "2              NaN      NaN   \n",
       "3              NaN      NaN   \n",
       "4              NaN      NaN   \n",
       "\n",
       "                                      pdf_json_files  \\\n",
       "0  document_parses/pdf_json/d1aafb70c066a2068b027...   \n",
       "1  document_parses/pdf_json/6b0567729c2143a66d737...   \n",
       "2  document_parses/pdf_json/06ced00a5fc04215949aa...   \n",
       "3  document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
       "4  document_parses/pdf_json/5f48792a5fa08bed9f560...   \n",
       "\n",
       "                               pmc_json_files  \\\n",
       "0  document_parses/pmc_json/PMC35282.xml.json   \n",
       "1  document_parses/pmc_json/PMC59543.xml.json   \n",
       "2  document_parses/pmc_json/PMC59549.xml.json   \n",
       "3  document_parses/pmc_json/PMC59574.xml.json   \n",
       "4  document_parses/pmc_json/PMC59580.xml.json   \n",
       "\n",
       "                                                 url  s2_id  \n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...    NaN  \n",
       "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
       "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
       "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Column names:\n",
      " 1. cord_uid\n",
      " 2. sha\n",
      " 3. source_x\n",
      " 4. title\n",
      " 5. doi\n",
      " 6. pmcid\n",
      " 7. pubmed_id\n",
      " 8. license\n",
      " 9. abstract\n",
      "10. publish_time\n",
      "11. authors\n",
      "12. journal\n",
      "13. mag_id\n",
      "14. who_covidence_id\n",
      "15. arxiv_id\n",
      "16. pdf_json_files\n",
      "17. pmc_json_files\n",
      "18. url\n",
      "19. s2_id\n",
      "\n",
      " Dataset dimensions: 1056660 rows × 19 columns\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n Column names:\")\n",
    "\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\n Dataset dimensions: {df.shape[0]} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d47da90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Data types of each column:\n",
      "cord_uid                  object\n",
      "sha                       object\n",
      "source_x                  object\n",
      "title                     object\n",
      "doi                       object\n",
      "pmcid                     object\n",
      "pubmed_id                 object\n",
      "license                   object\n",
      "abstract                  object\n",
      "publish_time              object\n",
      "authors                   object\n",
      "journal                   object\n",
      "mag_id                    float64\n",
      "who_covidence_id          object\n",
      "arxiv_id                  object\n",
      "pdf_json_files            object\n",
      "pmc_json_files            object\n",
      "url                       object\n",
      "s2_id                     float64\n",
      "\n",
      " General information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1056660 entries, 0 to 1056659\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count    Dtype  \n",
      "---  ------            --------------    -----  \n",
      " 0   cord_uid          1056660 non-null  object \n",
      " 1   sha               373766 non-null   object \n",
      " 2   source_x          1056660 non-null  object \n",
      " 3   title             1056157 non-null  object \n",
      " 4   doi               656780 non-null   object \n",
      " 5   pmcid             389571 non-null   object \n",
      " 6   pubmed_id         498932 non-null   object \n",
      " 7   license           1056660 non-null  object \n",
      " 8   abstract          821116 non-null   object \n",
      " 9   publish_time      1054846 non-null  object \n",
      " 10  authors           1032791 non-null  object \n",
      " 11  journal           969338 non-null   object \n",
      " 12  mag_id            0 non-null        float64\n",
      " 13  who_covidence_id  482935 non-null   object \n",
      " 14  arxiv_id          14249 non-null    object \n",
      " 15  pdf_json_files    373766 non-null   object \n",
      " 16  pmc_json_files    315742 non-null   object \n",
      " 17  url               686934 non-null   object \n",
      " 18  s2_id             976468 non-null   float64\n",
      "dtypes: float64(2), object(17)\n",
      "memory usage: 153.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print(\"  Data types of each column:\")\n",
    "\n",
    "data_types = df.dtypes\n",
    "for col, dtype in data_types.items():\n",
    "    print(f\"{col:<25} {dtype}\")\n",
    "\n",
    "print(\"\\n General information:\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18257ed",
   "metadata": {},
   "source": [
    "## Section 4: Data Quality Assessment\n",
    "\n",
    "Now let's assess the quality of our data by checking for missing values and generating basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df8c975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Missing values analysis:\n",
      " Columns with missing values: 16 out of 19\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\" Missing values analysis:\")\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "\n",
    "# Create a summary dataframe\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing_Count': missing_data.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "# Sort by missing percentage\n",
    "missing_summary = missing_summary.sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "# Display columns with missing values\n",
    "missing_cols = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "print(f\" Columns with missing values: {len(missing_cols)} out of {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db7b342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE:\n",
      "  Non-null values: 1,056,157\n",
      "  Unique values: 850,366\n",
      "\n",
      "ABSTRACT:\n",
      "  Non-null values: 821,116\n",
      "  Unique values: 730,712\n",
      "\n",
      "PUBLISH_TIME:\n",
      "  Non-null values: 1,054,846\n",
      "  Unique values: 8,056\n",
      "  Date range: 1856-04-01 00:00:00 to 2024-04-20 00:00:00\n",
      "\n",
      "JOURNAL:\n",
      "  Non-null values: 969,338\n",
      "  Unique values: 54,993\n",
      "\n",
      "AUTHORS:\n",
      "  Non-null values: 1,032,791\n",
      "  Unique values: 796,659\n"
     ]
    }
   ],
   "source": [
    "# Generate basic statistics for key columns\n",
    "\n",
    "# Focus on key columns that are likely to be important\n",
    "key_columns = ['title', 'abstract', 'publish_time', 'journal', 'authors']\n",
    "\n",
    "for col in key_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(f\"  Non-null values: {df[col].notna().sum():,}\")\n",
    "        print(f\"  Unique values: {df[col].nunique():,}\")\n",
    "        if col == 'publish_time':\n",
    "            # Try to get date range\n",
    "            try:\n",
    "                dates = pd.to_datetime(df[col], errors='coerce')\n",
    "                print(f\"  Date range: {dates.min()} to {dates.max()}\")\n",
    "            except:\n",
    "                print(\"  Date conversion failed\")\n",
    "    else:\n",
    "        print(f\"\\n{col.upper()}: Column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94afbcd9",
   "metadata": {},
   "source": [
    "## Section 5: Handle Missing Data\n",
    "\n",
    "Based on our analysis, we'll create a cleaned version of the dataset by handling missing values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e13f3f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 1056660 rows, 19 columns\n",
      "After removing rows without titles: 1056157 rows\n",
      "Final cleaned dataset: 1056157 rows, 19 columns\n",
      "Rows removed: 503 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Create a cleaned version of the dataset\n",
    "\n",
    "# Start with a copy of the original data\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(f\"Original dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# For our analysis, we need papers with titles and some publication info\n",
    "# Remove rows where title is missing (these are not useful for our analysis)\n",
    "if 'title' in df_clean.columns:\n",
    "    df_clean = df_clean.dropna(subset=['title'])\n",
    "    print(f\"After removing rows without titles: {df_clean.shape[0]} rows\")\n",
    "\n",
    "# For publish_time, we'll keep all rows but handle missing dates later\n",
    "# For other columns, we'll fill missing values with appropriate defaults\n",
    "\n",
    "# Fill missing journal with 'Unknown'\n",
    "if 'journal' in df_clean.columns:\n",
    "    df_clean['journal'] = df_clean['journal'].fillna('Unknown')\n",
    "\n",
    "# Fill missing abstract with empty string\n",
    "if 'abstract' in df_clean.columns:\n",
    "    df_clean['abstract'] = df_clean['abstract'].fillna('')\n",
    "\n",
    "print(f\"Final cleaned dataset: {df_clean.shape[0]} rows, {df_clean.shape[1]} columns\")\n",
    "print(f\"Rows removed: {df.shape[0] - df_clean.shape[0]} ({((df.shape[0] - df_clean.shape[0]) / df.shape[0] * 100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6dfa2",
   "metadata": {},
   "source": [
    "## Section 6: Data Type Conversions and Feature Engineering\n",
    "\n",
    "Let's convert date columns and create new features for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "414b9b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing dates: 537728\n",
      "Date range: 1856-04-01 00:00:00 to 2024-04-20 00:00:00\n",
      "Year range: 1856.0 to 2024.0\n",
      "Title length - Mean: 99.1, Max: 1504\n",
      "Abstract word count - Mean: 164.7, Max: 18000\n",
      "\n",
      " Feature engineering complete! Dataset now has 23 columns.\n"
     ]
    }
   ],
   "source": [
    "# Convert publish_time to datetime and extract year\n",
    "\n",
    "# Convert publish_time to datetime\n",
    "if 'publish_time' in df_clean.columns:\n",
    "    df_clean['publish_date'] = pd.to_datetime(df_clean['publish_time'], errors='coerce')\n",
    "    \n",
    "    # Extract year from publication date\n",
    "    df_clean['publish_year'] = df_clean['publish_date'].dt.year\n",
    "    \n",
    "    # For missing dates, we can try to extract year from the string if possible\n",
    "    missing_dates = df_clean['publish_date'].isna()\n",
    "    print(f\"Rows with missing dates: {missing_dates.sum()}\")\n",
    "    \n",
    "    # Check the date range\n",
    "    valid_dates = df_clean['publish_date'].dropna()\n",
    "    if len(valid_dates) > 0:\n",
    "        print(f\"Date range: {valid_dates.min()} to {valid_dates.max()}\")\n",
    "        print(f\"Year range: {df_clean['publish_year'].min()} to {df_clean['publish_year'].max()}\")\n",
    "\n",
    "# Create new features\n",
    "# Title length\n",
    "if 'title' in df_clean.columns:\n",
    "    df_clean['title_length'] = df_clean['title'].str.len()\n",
    "    print(f\"Title length - Mean: {df_clean['title_length'].mean():.1f}, Max: {df_clean['title_length'].max()}\")\n",
    "\n",
    "# Abstract word count\n",
    "if 'abstract' in df_clean.columns:\n",
    "    df_clean['abstract_word_count'] = df_clean['abstract'].str.split().str.len()\n",
    "    # Handle empty abstracts\n",
    "    df_clean['abstract_word_count'] = df_clean['abstract_word_count'].fillna(0)\n",
    "    print(f\"Abstract word count - Mean: {df_clean['abstract_word_count'].mean():.1f}, Max: {df_clean['abstract_word_count'].max()}\")\n",
    "\n",
    "print(f\"\\n Feature engineering complete! Dataset now has {df_clean.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290eccf9",
   "metadata": {},
   "source": [
    "## Section 7: Publication Trends Analysis\n",
    "\n",
    "Let's analyze publication trends over time to understand how COVID-19 research evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7680f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers published by year:\n",
      "1856.0: 3 papers\n",
      "1857.0: 1 papers\n",
      "1860.0: 2 papers\n",
      "1864.0: 1 papers\n",
      "1876.0: 1 papers\n",
      "1878.0: 1 papers\n",
      "1879.0: 1 papers\n",
      "1900.0: 1 papers\n",
      "1902.0: 1 papers\n",
      "1903.0: 1 papers\n",
      "1955.0: 1 papers\n",
      "1957.0: 2 papers\n",
      "1961.0: 1 papers\n",
      "1962.0: 1 papers\n",
      "1963.0: 1 papers\n",
      "1964.0: 2 papers\n",
      "1965.0: 1 papers\n",
      "1967.0: 1 papers\n",
      "1968.0: 1 papers\n",
      "1969.0: 3 papers\n",
      "1970.0: 10 papers\n",
      "1971.0: 8 papers\n",
      "1972.0: 13 papers\n",
      "1973.0: 11 papers\n",
      "1974.0: 14 papers\n",
      "1975.0: 20 papers\n",
      "1976.0: 17 papers\n",
      "1977.0: 24 papers\n",
      "1978.0: 26 papers\n",
      "1979.0: 19 papers\n",
      "1980.0: 31 papers\n",
      "1981.0: 42 papers\n",
      "1982.0: 39 papers\n",
      "1983.0: 38 papers\n",
      "1984.0: 68 papers\n",
      "1985.0: 73 papers\n",
      "1986.0: 82 papers\n",
      "1987.0: 89 papers\n",
      "1988.0: 94 papers\n",
      "1989.0: 108 papers\n",
      "1990.0: 116 papers\n",
      "1991.0: 124 papers\n",
      "1992.0: 156 papers\n",
      "1993.0: 103 papers\n",
      "1994.0: 104 papers\n",
      "1995.0: 101 papers\n",
      "1996.0: 101 papers\n",
      "1997.0: 112 papers\n",
      "1998.0: 152 papers\n",
      "1999.0: 182 papers\n",
      "2000.0: 212 papers\n",
      "2001.0: 200 papers\n",
      "2002.0: 703 papers\n",
      "2003.0: 835 papers\n",
      "2004.0: 1,632 papers\n",
      "2005.0: 1,553 papers\n",
      "2006.0: 1,744 papers\n",
      "2007.0: 1,682 papers\n",
      "2008.0: 2,190 papers\n",
      "2009.0: 2,542 papers\n",
      "2010.0: 2,223 papers\n",
      "2011.0: 2,337 papers\n",
      "2012.0: 2,471 papers\n",
      "2013.0: 2,948 papers\n",
      "2014.0: 3,207 papers\n",
      "2015.0: 3,461 papers\n",
      "2016.0: 3,944 papers\n",
      "2017.0: 3,691 papers\n",
      "2018.0: 3,982 papers\n",
      "2019.0: 5,629 papers\n",
      "2020.0: 164,537 papers\n",
      "2021.0: 219,335 papers\n",
      "2022.0: 85,265 papers\n",
      "2023.0: 1 papers\n",
      "2024.0: 1 papers\n",
      "\n",
      "COVID-19 era (2019+): 474,768 papers\n",
      "\n",
      " Monthly publication trends (2020-2022):\n",
      "Peak month: 2021-03 with 20,381 papers\n",
      "\n",
      "Top 5 months by publication count:\n",
      "  2021-03: 20,381 papers\n",
      "  2022-03: 19,863 papers\n",
      "  2021-05: 19,696 papers\n",
      "  2021-06: 19,460 papers\n",
      "  2020-09: 19,375 papers\n",
      "Peak month: 2021-03 with 20,381 papers\n",
      "\n",
      "Top 5 months by publication count:\n",
      "  2021-03: 20,381 papers\n",
      "  2022-03: 19,863 papers\n",
      "  2021-05: 19,696 papers\n",
      "  2021-06: 19,460 papers\n",
      "  2020-09: 19,375 papers\n"
     ]
    }
   ],
   "source": [
    "# Count papers by publication year\n",
    "\n",
    "if 'publish_year' in df_clean.columns:\n",
    "    # Count papers by year\n",
    "    yearly_counts = df_clean['publish_year'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"Papers published by year:\")\n",
    "    for year, count in yearly_counts.items():\n",
    "        if pd.notna(year):\n",
    "            print(f\"{year}: {count:,} papers\")\n",
    "    \n",
    "    # Focus on recent years (COVID-19 era)\n",
    "    covid_years = yearly_counts[yearly_counts.index >= 2019]\n",
    "    print(f\"\\nCOVID-19 era (2019+): {covid_years.sum():,} papers\")\n",
    "    \n",
    "    # Store for visualization\n",
    "    yearly_data = yearly_counts.reset_index()\n",
    "    yearly_data.columns = ['Year', 'Paper_Count']\n",
    "else:\n",
    "    print(\"No publication year data available\")\n",
    "\n",
    "# Analyze monthly trends for recent years\n",
    "if 'publish_date' in df_clean.columns:\n",
    "    print(\"\\n Monthly publication trends (2020-2022):\")\n",
    "    recent_papers = df_clean[df_clean['publish_year'].between(2020, 2022)]\n",
    "    if len(recent_papers) > 0:\n",
    "        # Create temporary year and month columns for grouping\n",
    "        recent_papers_copy = recent_papers.copy()\n",
    "        recent_papers_copy['year'] = recent_papers_copy['publish_date'].dt.year\n",
    "        recent_papers_copy['month'] = recent_papers_copy['publish_date'].dt.month\n",
    "        \n",
    "        monthly_counts = recent_papers_copy.groupby(['year', 'month']).size().reset_index(name='paper_count')\n",
    "        \n",
    "        # Find peak months\n",
    "        if len(monthly_counts) > 0:\n",
    "            peak_month = monthly_counts.loc[monthly_counts['paper_count'].idxmax()]\n",
    "            print(f\"Peak month: {peak_month['year']}-{peak_month['month']:02d} with {peak_month['paper_count']:,} papers\")\n",
    "            \n",
    "            # Show top 5 months\n",
    "            top_months = monthly_counts.nlargest(5, 'paper_count')\n",
    "            print(\"\\nTop 5 months by publication count:\")\n",
    "            for _, row in top_months.iterrows():\n",
    "                print(f\"  {row['year']}-{row['month']:02d}: {row['paper_count']:,} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e6a92",
   "metadata": {},
   "source": [
    "## Section 8: Journal and Source Analysis\n",
    "\n",
    "Let's identify the top journals and sources publishing COVID-19 research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d234ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top journals publishing COVID-19 research\n",
    "\n",
    "\n",
    "if 'journal' in df_clean.columns:\n",
    "    # Count papers by journal\n",
    "    journal_counts = df_clean['journal'].value_counts()\n",
    "    \n",
    "    print(\"Top 15 journals by paper count:\")\n",
    "    for i, (journal, count) in enumerate(journal_counts.head(15).items(), 1):\n",
    "        print(f\"{i:2d}. {journal:<50} {count:>6,} papers\")\n",
    "    \n",
    "    # Store top journals for visualization\n",
    "    top_journals = journal_counts.head(10).reset_index()\n",
    "    top_journals.columns = ['Journal', 'Paper_Count']\n",
    "    \n",
    "    print(f\"\\nTotal unique journals: {len(journal_counts)}\")\n",
    "    print(f\"Papers in top 10 journals: {top_journals['Paper_Count'].sum():,} ({top_journals['Paper_Count'].sum()/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# Check if there are other source columns\n",
    "potential_source_cols = [col for col in df_clean.columns if 'source' in col.lower()]\n",
    "if potential_source_cols:\n",
    "    print(f\"\\n Other source columns found: {potential_source_cols}\")\n",
    "    \n",
    "    for col in potential_source_cols[:2]:  # Analyze first 2 source columns\n",
    "        print(f\"\\nTop sources in '{col}':\")\n",
    "        source_counts = df_clean[col].value_counts()\n",
    "        for i, (source, count) in enumerate(source_counts.head(10).items(), 1):\n",
    "            print(f\"{i:2d}. {source:<40} {count:>6,}\")\n",
    "else:\n",
    "    print(\"No additional source columns found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f6395",
   "metadata": {},
   "source": [
    "## Section 9: Text Analysis of Paper Titles\n",
    "\n",
    "Let's analyze the most common words and themes in COVID-19 research paper titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ef4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze word frequency in paper titles\n",
    "\n",
    "if 'title' in df_clean.columns:\n",
    "    # Get all titles and convert to lowercase\n",
    "    all_titles = df_clean['title'].dropna().str.lower()\n",
    "    \n",
    "    # Split titles into words and count frequency\n",
    "    all_words = []\n",
    "    for title in all_titles:\n",
    "        # Simple word extraction (remove punctuation and split)\n",
    "        words = title.replace(',', ' ').replace('.', ' ').replace(':', ' ').replace(';', ' ').replace('(', ' ').replace(')', ' ').split()\n",
    "        # Filter out very short words and common stop words\n",
    "        stop_words = {'the', 'and', 'of', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'with', 'by', 'from', 'as', 'is', 'are', 'was', 'were', 'be', 'been', 'being'}\n",
    "        words = [word.strip('.,!?:;()[]{}') for word in words if len(word) > 2 and word not in stop_words]\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(all_words)\n",
    "    \n",
    "    print(\"Top 20 most frequent words in titles:\")\n",
    "    for i, (word, count) in enumerate(word_counts.most_common(20), 1):\n",
    "        print(f\"{i:2d}. {word:<15} {count:>6,} times\")\n",
    "    \n",
    "    # Store for word cloud\n",
    "    top_words = dict(word_counts.most_common(50))\n",
    "    \n",
    "    # Analyze COVID-related terms\n",
    "    covid_terms = ['covid', 'coronavirus', 'sars', 'pandemic', 'vaccine', 'vaccination']\n",
    "    print(f\"\\nCOVID-19 related terms in titles:\")\n",
    "    for term in covid_terms:\n",
    "        count = word_counts.get(term, 0)\n",
    "        if count > 0:\n",
    "            print(f\"  {term}: {count:,} times\")\n",
    "    \n",
    "    print(f\"\\nTotal unique words: {len(word_counts)}\")\n",
    "    print(f\"Total words analyzed: {len(all_words):,}\")\n",
    "else:\n",
    "    print(\"No title data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6213cbe",
   "metadata": {},
   "source": [
    "## Section 10: Create Data Visualizations\n",
    "\n",
    "Now let's create visualizations to better understand our data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication trends visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 1. Publications over time\n",
    "plt.subplot(2, 2, 1)\n",
    "if 'yearly_data' in locals():\n",
    "    # Filter for reasonable years\n",
    "    yearly_filtered = yearly_data[yearly_data['Year'].between(2000, 2024)]\n",
    "    plt.plot(yearly_filtered['Year'], yearly_filtered['Paper_Count'], marker='o', linewidth=2, markersize=6)\n",
    "    plt.title('COVID-19 Research Publications by Year', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# 2. Top journals bar chart\n",
    "plt.subplot(2, 2, 2)\n",
    "if 'top_journals' in locals():\n",
    "    # Get top 8 journals for better readability\n",
    "    top_8_journals = top_journals.head(8)\n",
    "    bars = plt.bar(range(len(top_8_journals)), top_8_journals['Paper_Count'], color='skyblue', edgecolor='navy')\n",
    "    plt.title('Top Journals Publishing COVID-19 Research', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Journals')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.xticks(range(len(top_8_journals)), [j[:20] + '...' if len(j) > 20 else j for j in top_8_journals['Journal']], \n",
    "               rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{int(height):,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 3. Title length distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "if 'title_length' in df_clean.columns:\n",
    "    title_lengths = df_clean['title_length'].dropna()\n",
    "    plt.hist(title_lengths, bins=50, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "    plt.title('Distribution of Paper Title Lengths', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Title Length (characters)')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.axvline(title_lengths.mean(), color='red', linestyle='--', label=f'Mean: {title_lengths.mean():.1f}')\n",
    "    plt.legend()\n",
    "\n",
    "# 4. Abstract word count distribution\n",
    "plt.subplot(2, 2, 4)\n",
    "if 'abstract_word_count' in df_clean.columns:\n",
    "    abstract_counts = df_clean['abstract_word_count'].dropna()\n",
    "    # Filter out extreme outliers for better visualization\n",
    "    filtered_counts = abstract_counts[abstract_counts <= abstract_counts.quantile(0.95)]\n",
    "    plt.hist(filtered_counts, bins=50, color='orange', edgecolor='darkorange', alpha=0.7)\n",
    "    plt.title('Distribution of Abstract Word Counts', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Abstract Word Count')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.axvline(filtered_counts.mean(), color='red', linestyle='--', label=f'Mean: {filtered_counts.mean():.1f}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a33f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple word cloud visualization using matplotlib\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Word cloud alternative using bar chart\n",
    "plt.subplot(1, 2, 1)\n",
    "if 'top_words' in locals():\n",
    "    # Get top 15 words for better readability\n",
    "    top_15_words = dict(list(top_words.items())[:15])\n",
    "    words = list(top_15_words.keys())\n",
    "    counts = list(top_15_words.values())\n",
    "    \n",
    "    bars = plt.barh(range(len(words)), counts, color='lightcoral', edgecolor='darkred')\n",
    "    plt.title('Most Frequent Words in Paper Titles', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.gca().invert_yaxis()  # Most frequent at top\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + width*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{int(width):,}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "# COVID-19 related terms visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "if 'word_counts' in locals():\n",
    "    covid_related = ['covid', 'coronavirus', 'sars', 'pandemic', 'vaccine', 'vaccination', 'treatment', 'patient', 'clinical']\n",
    "    covid_data = [(word, word_counts.get(word, 0)) for word in covid_related if word_counts.get(word, 0) > 0]\n",
    "    \n",
    "    if covid_data:\n",
    "        covid_words, covid_counts = zip(*covid_data)\n",
    "        bars = plt.bar(range(len(covid_words)), covid_counts, color='steelblue', edgecolor='navy')\n",
    "        plt.title('COVID-19 Related Terms in Titles', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Terms')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(range(len(covid_words)), covid_words, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{int(height):,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "\n",
    "# Saving only a sample to avaoid large file sizes\n",
    "df_clean.sample(frac=0.1).to_csv('covid19_cleaned_sample.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
